{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine-tuning the pRoBERTa model for PPI prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Process Input Data\n",
    "The input data are two datasets in csv format containing experimentally confirmed protein-protein interactions which will be called the\n",
    "positive dataset and random protein sequences pairs that will be called the negative datasets. These datasets should have\n",
    "the following columns ```HUMAN_SEQ, VIRUS_SEQ``` designating the columns containing the sequences from the humans and\n",
    "viruses respectively.\n",
    "\n",
    "The datasets are labeled positive and negative then concatenated to one dataset.\n",
    "\n",
    "### 1.1 Alternative\n",
    "The input dataset can be an excel file containing a positive and negative sheet."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def from_csv(positive_path, negative_path):\n",
    "    positive_dataframe = pd.read_csv(positive_path, usecols=['HUMAN_SEQ', 'VIRUS_SEQ'])\n",
    "    positive_dataframe['label'] = 'positive'\n",
    "    negative_dataframe = pd.read_csv(negative_path, usecols=['HUMAN_SEQ', 'VIRUS_SEQ'])\n",
    "    negative_dataframe['label'] = 'negative'\n",
    "    output_dataframe = pd.concat([positive_dataframe, negative_dataframe])\n",
    "    output_dataframe = output_dataframe.rename(columns={'HUMAN_SEQ':'from', 'VIRUS_SEQ':'to'})\n",
    "    return output_dataframe\n",
    "\n",
    "def from_excel(filepath, positive_sheet, negative_sheet):\n",
    "    positive_dataframe = pd.read_excel(filepath, sheet_name=positive_sheet)\n",
    "    negative_dataframe = pd.read_excel(filepath, sheet_name=negative_sheet)\n",
    "    positive_dataframe['label'] = 'positive'\n",
    "    negative_dataframe['label'] = 'negative'\n",
    "    output_dataframe = pd.concat([positive_dataframe, negative_dataframe])\n",
    "    output_dataframe = output_dataframe.rename(columns={'HUMAN_SEQ': 'from', 'VIRUS_SEQ': 'to'})\n",
    "    return output_dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "positive = 'Data/h1n1_data/training_set/H1N1_human_pos_training.csv'\n",
    "negative = 'Data/h1n1_data/training_set/H1N1_human_neg_training.csv'\n",
    "\n",
    "df = from_csv(positive, negative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Tokenize the protein sequences using a pretrained BPE model\n",
    "The model was trained previously on the uniprot database of protein sequences based on the BPE algorithm using SentencePiece\n",
    "The HUMAN_SEQ and VIRUS_SEQ columns was changed to ```from``` and ```to```.\n",
    "\n",
    "The tokenized sequences were saved in a csv file for future use or as a variable for continued processing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "model_path = 'BPE_model/m_reviewed.model'\n",
    "model = spm.SentencePieceProcessor()\n",
    "model.load(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(dataframe, model):\n",
    "    dfv = dataframe[['from', 'to', 'label']].values\n",
    "    out = []\n",
    "    for row in dfv:\n",
    "        out.append([\" \".join(model.encode_as_pieces(row[0])), \" \".join(model.encode_as_pieces(row[1])),row[2]])\n",
    "    print(out[101])\n",
    "    out_df = pd.DataFrame(out, columns=['from', 'to', 'label'])\n",
    "    print(out_df.head())\n",
    "    out_df.to_csv('Data/H1N1_interact_tokenized_full.csv', index=False)\n",
    "    return out_df\n",
    "\n",
    "tokenized_df = tokenize(df, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Shuffling and Dataset Split\n",
    "The tokenized dataframe is shuffled using ```sample(frac=1)``` which samples the dataset and returns 100% of the dataset\n",
    "shuffled.\n",
    "\n",
    "The dataframe is then sliced into 80%, 10% and 10% slices, for train, validation and test sets.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def shuffle_dataframe(out_put_df):\n",
    "    out_put_df_shuffled = out_put_df.sample(frac=1)\n",
    "    train = math.ceil(len(out_put_df_shuffled) * 0.8)\n",
    "    test = math.ceil(len(out_put_df_shuffled) * 0.8) + math.ceil(len(out_put_df_shuffled) * 0.1)\n",
    "\n",
    "    shuffled_80_train = out_put_df_shuffled[0:train]\n",
    "    shuffled_10_valid = out_put_df_shuffled[train:test]\n",
    "    shuffled_10_test = out_put_df_shuffled[test:]\n",
    "\n",
    "    return shuffled_80_train, shuffled_10_valid, shuffled_10_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_shuffled_80_train, tokenized_shuffled_10_valid, tokenized_shuffled_10_test = shuffle_dataframe(tokenized_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Export the split datasets\n",
    "The each column from the split datasets is extracted and converted to lists. These are then written to the disk."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3e30db170a2b>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-1-3e30db170a2b>\"\u001B[0;36m, line \u001B[0;32m24\u001B[0m\n\u001B[0;31m    if item == 'label'\u001B[0m\n\u001B[0m                      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# from sequences\n",
    "\n",
    "def splits_to_list():\n",
    "    from_to_label_list = ['from', 'to', 'label']\n",
    "    for item in from_to_label_list:\n",
    "        if item == 'from':\n",
    "            from_trained = tokenized_shuffled_80_train[item].tolist()\n",
    "            from_valid = tokenized_shuffled_10_valid[item].tolist()\n",
    "            from_test = tokenized_shuffled_10_test[item].tolist()\n",
    "            from_dict = {'from_train':from_trained, 'from_valid':from_valid, 'from_test':from_test}\n",
    "            for k,v in from_dict.items():\n",
    "                with open(f'split_tokenized/from/{k}.txt', 'w') as f:\n",
    "                    for it in v:\n",
    "                        f.write('%s\\n' % it)\n",
    "        if item == 'to':\n",
    "            to_trained = tokenized_shuffled_80_train['to'].tolist()\n",
    "            to_valid  = tokenized_shuffled_10_valid['to'].tolist()\n",
    "            to_test = tokenized_shuffled_10_test['to'].tolist()\n",
    "            to_dict = {'to_train':to_trained, 'to_valid':to_valid,'to_test':to_test}\n",
    "            for k,v in to_dict.items():\n",
    "                with open(f'split_tokenized/to/{k}.txt', 'w') as f:\n",
    "                    for it in v:\n",
    "                        f.write('%s\\n' % it)\n",
    "        if item == 'label':\n",
    "            label_trained =tokenized_shuffled_80_train['label'].tolist()\n",
    "            label_valid = tokenized_shuffled_10_valid['label'].tolist()\n",
    "            label_test = tokenized_shuffled_10_test['label'].tolist()\n",
    "            label_dict = {'label_train':label_trained, 'label_valid':label_valid,'label_test':label_test}\n",
    "            for k,v in label_dict.items():\n",
    "                with open(f'split_tokenized/label/{k}.txt', 'w') as f:\n",
    "                    for it in v:\n",
    "                        f.write('%s\\n' % it)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Fine-tune the ProBERTa model to predict PPIs\n",
    "As a test I used the pretrained model to fine tune on human-human PPI data. That model I fine-tuned I used to further fine-tune\n",
    "a human-H1N1 ppi predictor.\n",
    "\n",
    "Arguments:\n",
    "ppi - model file prefix --PREFIX\n",
    "\n",
    "int (1) - number of gpus --NUM_GPUS\n",
    "\n",
    "ppi_finetune - destination directory --OUTPUT_DIR\n",
    "\n",
    "split_binarized - input directory --DATA_DIR\n",
    "\n",
    "768 - Dimension of embedding generated by the encoders --ENCODER_EMBED_DIM\n",
    "\n",
    "5 - Number of encoder layers in the model --ENCODER_LAYERS\n",
    "\n",
    "125000 - Maximum number of updates during training --TOTAL_UPDATES\n",
    "3125 - Total number of Learning Rate warm-up updates during training --WARMUP_UPDATES\n",
    "0.0025 - Peak learning rate for training --PEAK_LEARNING_RATE\n",
    "32 - Maximum number of sequences in each batch --MAX_SENTENCES\n",
    "64 - Updates the model every UPDATE_FREQ batches --UPDATE_FREQ\n",
    "3  - Early stop training if valid performance does not improve for PATIENCS consecutive validation runs --PATIENCE\n",
    "checkpoint.pt - Path to pretrained model checkpoint --PRETRAIN_CHECKPOINT\n",
    "no- Whether to resume training from previous finetuned model checkpoints -- RESUME_TRAINING\n",
    "True - To use [cls] token\n",
    "\n",
    "These arguments are used to initiate ```fairseq-train``` with the following arguments:\n",
    "\n",
    "```\n",
    "if [ \"$RESUME\" = \"no\" ]; then\n",
    "    fairseq-train --fp16 --fp16-no-flatten-grads $DATA_DIR \\\n",
    "        --max-positions $MAX_POSITIONS --max-sentences $MAX_SENTENCES \\\n",
    "        --arch roberta_base --task sentence_prediction \\\n",
    "        --truncate-sequence --use-cls-token $USE_CLS \\\n",
    "\t      --bpe sentencepiece \\\n",
    "        --classification-head-name protein_interaction_prediction \\\n",
    "        --restore-file \"$ROBERTA_PATH\" --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "        --init-token 0 --separator-token 2 \\\n",
    "        --criterion sentence_prediction --num-classes $NUM_CLASSES \\\n",
    "        --optimizer lamb \\\n",
    "        --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --update-freq $UPDATE_FREQ \\\n",
    "        --max-update $TOTAL_UPDATES \\\n",
    "        --encoder-embed-dim $ENCODER_EMBED_DIM --encoder-layers $ENCODER_LAYERS \\\n",
    "        --save-dir \"$CHECKPOINT_DIR\" --save-interval 1 --save-interval-updates 100 --keep-interval-updates 5 \\\n",
    "        --distributed-world-size $NUM_GPUS --ddp-backend=no_c10d \\\n",
    "        --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
    "        --patience $PATIENCE \\\n",
    "        --log-format simple --log-interval 1000 2>&1 | tee -a \"$LOG_FILE\"\n",
    "else\n",
    "    fairseq-train --fp16 --fp16-no-flatten-grads $DATA_DIR \\\n",
    "        --max-positions $MAX_POSITIONS --max-sentences $MAX_SENTENCES \\\n",
    "        --arch roberta_base --task sentence_prediction \\\n",
    "        --truncate-sequence --use-cls-token $USE_CLS \\\n",
    "        --bpe sentencepiece \\\n",
    "        --classification-head-name protein_interaction_prediction \\\n",
    "        --init-token 0 --separator-token 2 \\\n",
    "        --criterion sentence_prediction --num-classes $NUM_CLASSES \\\n",
    "        --optimizer lamb \\\n",
    "        --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --update-freq $UPDATE_FREQ \\\n",
    "        --max-update $TOTAL_UPDATES \\\n",
    "        --encoder-embed-dim $ENCODER_EMBED_DIM --encoder-layers $ENCODER_LAYERS \\\n",
    "        --save-dir \"$CHECKPOINT_DIR\" --save-interval 1 --save-interval-updates 100 --keep-interval-updates 5 \\\n",
    "        --distributed-world-size $NUM_GPUS --ddp-backend=no_c10d \\\n",
    "        --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
    "        --patience $PATIENCE \\\n",
    "        --log-format simple --log-interval 1000 2>&1 | tee -a \"$LOG_FILE\"\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "pRoBERTa_finetune_ppi.sh ppi 1 ppi_finetune split_binarized/ 768 5 12500 312 0.0025 32 64 2 3 ppi_prediction/ppi.DIM_768.LAYERS_5.UPDATES_12500.WARMUP_312.LR_0.0025.BATCH_2048.PATIENCE_3/checkpoints/checkpoint_best.pt no True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Evaluate the model\n",
    "We use the test data to evaluate the effectiveness of the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "from scipy.special import softmax\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path to input tokenized data: from, to, label\n",
    "\n",
    "\n",
    "data_path = ''\n",
    "binarized_path = ''\n",
    "output_path = ''\n",
    "model_folder = ''\n",
    "classificatin_head = ''\n",
    "batch_size = int('')\n",
    "\n",
    "has_cuda = torch.cuda.device_count() > 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from_col = 0\n",
    "to_col = 1\n",
    "label_col = 2\n",
    "tuple_col = 3\n",
    "softmax_col = 4\n",
    "pred_col = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, header=None)\n",
    "data[label_col] = data[label_col].str.replace(\" \", \"_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained(model_folder, \"checkpoint_best.pt\", binarized_path, bpe=None)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if (has_cuda):\n",
    "    model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split_num = int(len(data)/batch_size)\n",
    "batch_data = np.array_split(data, split_num)\n",
    "print(f\"Total batches:{len(batch_data)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}